#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ—…æ¸¸çŸ­è§†é¢‘è¯­è¨€é£æ ¼æ£€æµ‹å™¨
åŠŸèƒ½ï¼šæ‰¹é‡é‡åŒ–æ—…æ¸¸çŸ­è§†é¢‘çš„è¯­è¨€é£æ ¼ï¼ˆåŠŸèƒ½å‹/æƒ…æ„Ÿå‹/æ„è±¡å‹ï¼‰
"""

import pandas as pd
import numpy as np
import re
import jieba
import os
import time
from typing import Optional, Tuple, List
import warnings
from tqdm import tqdm
import ctypes
from ctypes import windll
import threading
warnings.filterwarnings('ignore')

class PreventSleep:
    """é˜²æ­¢ç³»ç»Ÿä¼‘çœ çš„ç±»"""
    ES_CONTINUOUS = 0x80000000
    ES_SYSTEM_REQUIRED = 0x00000001
    
    def __init__(self):
        self.is_active = False
    
    def enable(self):
        """å¯ç”¨é˜²ä¼‘çœ æ¨¡å¼"""
        if not self.is_active:
            try:
                windll.kernel32.SetThreadExecutionState(
                    self.ES_CONTINUOUS | self.ES_SYSTEM_REQUIRED
                )
                self.is_active = True
                print("å·²å¯ç”¨é˜²ä¼‘çœ æ¨¡å¼")
            except Exception as e:
                print(f"å¯ç”¨é˜²ä¼‘çœ æ¨¡å¼å¤±è´¥: {str(e)}")
    
    def disable(self):
        """ç¦ç”¨é˜²ä¼‘çœ æ¨¡å¼"""
        if self.is_active:
            try:
                windll.kernel32.SetThreadExecutionState(self.ES_CONTINUOUS)
                self.is_active = False
                print("å·²æ¢å¤é»˜è®¤ç”µæºè®¾ç½®")
            except Exception as e:
                print(f"æ¢å¤é»˜è®¤ç”µæºè®¾ç½®å¤±è´¥: {str(e)}")

def get_last_processed_id(output_path: str) -> int:
    """è·å–ä¸Šæ¬¡å¤„ç†åˆ°çš„ID"""
    try:
        if os.path.exists(output_path):
            df = pd.read_csv(output_path)
            if not df.empty:
                return df['video_id'].max()
    except Exception:
        pass
    return 0

def is_text_emoji_or_symbol(text: str) -> bool:
    """åˆ¤æ–­æ–‡æœ¬æ˜¯å¦ä»…åŒ…å«emojiã€æ ‡ç‚¹å’Œç©ºæ ¼"""
    return bool(re.fullmatch(r'[\W\s_]+', text))

def is_negation_present(text: str) -> bool:
    """æ£€æµ‹æ˜¯å¦å­˜åœ¨å¦å®šè¯"""
    negation_words = {'ä¸', 'æ²¡', 'é', 'æ— ', 'è«', 'å‹¿', 'æœª', 'å¦', 'åˆ«', 'ç”­', 'ä¸è¦'}
    words = set(jieba.lcut(text))
    return bool(words & negation_words)

# æ‰©å……jiebaè¯å…¸
NETWORK_WORDS = [
    'é¡¶æµ', 'ç»ç»å­', 'æ— è¯­å­', 'ç ´é˜²', 'ä¸Šå¤´', 'å˜å˜å¥½', 'å¤ªçˆ±äº†',
    'å¤ªç‰›äº†', 'å¤ªç»äº†', 'å¤ªå¼ºäº†', 'å¤ªèµäº†', 'å¤ªæ£’äº†', 'å¤ªå¥½äº†',
    'å¥½å®¶ä¼™', 'æ•‘å‘½å•Š', 'ç‰›é­”ç‹', 'ç¥ä»™é¢œå€¼', 'ç»ç¾', 'ç‚¸è£‚'
]

for word in NETWORK_WORDS:
    jieba.add_word(word)

# å…³é”®è¯è¯å…¸ä¼˜åŒ–
# åŠŸèƒ½å‹å…³é”®è¯ï¼ˆå»é‡ã€ä¼˜åŒ–ï¼‰
FUNCTIONAL_KEYWORDS = [
    # äº¤é€šå‡ºè¡Œ
    'åœ°é“ç«™', 'å…¬äº¤ç«™', 'è‡ªé©¾æ¸¸', 'æ¢ä¹˜ç«™', 'å‡ºç«™å£', 'åœè½¦åœº', 'å‡ºç§Ÿè½¦', 'ç½‘çº¦è½¦', 
    'å¯¼èˆªåˆ°', 'è·¯çº¿å›¾', 'çº¿è·¯å›¾', 'ç­è½¦ç«™', 'æœºåœºå¤§å·´', 'é«˜é“ç«™', 'ç«è½¦ç«™', 'åŠ¨è½¦ç»„',
    'é£æœºåœº', 'ç¥¨åŠ¡å¤„', 'å®‰æ£€å£', 'äº¤é€šæ¢çº½', 'èˆªç­å·', 'å€™è½¦å®¤', 'è¡Œææ¶', 'æ‰˜è¿å¤„',
    'ç™»æœºå£', 'ä¸‹è½¦å¤„', 'ä¸Šè½¦ç‚¹', 'è½¬ä¹˜å¤„', 'æ¥é©³ç«™', 'å‘è½¦ç‚¹', 'åˆ°è¾¾å£', 'å‡ºå‘åŒº',
    'ç»ˆç‚¹ç«™', 'èµ·ç‚¹ç«™', 'è½¦æ¬¡å·', 'è½¦ç¥¨ä»·', 'äº¤é€šå·¥å…·', 'è·¯ç¨‹è¿œ', 'è€—æ—¶é•¿', 'é€”ç»åœ°',
    
    # é¢„ç®—ç›¸å…³
    'äººå‡', 'å®¢å•ä»·', 'é—¨ç¥¨', 'ç¥¨ä»·', 'æŠ˜æ‰£', 'å­¦ç”Ÿç¥¨', 'ä¼˜æƒ ', 'å…è´¹', 'æ”¶è´¹', 'ä»·æ ¼',
    'å…ƒ', 'å—', 'é’±', 'è´¹ç”¨', 'æˆæœ¬', 'é¢„ç®—', 'æ¶ˆè´¹', 'èŠ±è´¹', 'ä¾¿å®œ', 'è´µ',
    'æ€§ä»·æ¯”', 'å›¢è´­', 'æ‹¼å•', 'AA', 'åˆ’ç®—', 'å®æƒ ', 'è¶…å€¼', 'ä¹°ä¸€é€ä¸€', 'èµ åˆ¸', 'è¿”ç°',
    'ç‰¹ä»·', 'é™æ—¶ä»·', 'ä½è‡³', 'æ‰“æŠ˜', 'æŠ˜ä¸ŠæŠ˜', 'æ—©é¸Ÿä»·', 'ä¼˜æƒ åˆ¸', 'æ»¡å‡', 'æ»¡é€',

    # æ”»ç•¥/ä½“éªŒ
    'æ”»ç•¥', 'é¿å‘', 'tips', 'è¡Œç¨‹', 'è¥ä¸šæ—¶é—´', 'å¼€æ”¾æ—¥', 'é—­é¦†', 'é¢„çº¦', 'æ’é˜Ÿ',
    'å»ºè®®', 'æ¨è', 'å¿…å»', 'å¿…çœ‹', 'å¿…åƒ', 'æ‰“å¡', 'æ¸¸ç©', 'è·¯å¾„', 'é¡ºåº',
    'å¹²è´§', 'çœé’±', 'çœå¿ƒ', 'æé†’', 'æ³¨æ„', 'é¡»çŸ¥', 'ä¸€å®šè¦', 'å¿…é¡»', 'ä¸èƒ½', 'å€¼å¾—',
    'ä½“éªŒ', 'å°è¯•', 'è¸©é›·', 'é˜²å‘', 'æµ‹è¯„', 'å®æµ‹', 'çœŸä½“éªŒ', 'ä¸Šæ‰‹', 'æ–°æ‰‹', 'å°ç™½',
    'è€æ‰‹', 'è¿›é˜¶', 'æ·±åº¦æ¸¸', 'æµ…å°', 'é€Ÿè§ˆ', 'é¡ºè·¯', 'é€†æ—¶é’ˆ', 'æ­£å¥½', 'æ–¹ä¾¿', 'å®‰æ’',
    'å®‰æ’è¡Œç¨‹', 'è¡Œç¨‹å•', 'è§„åˆ’', 'è·¯çº¿æ¨è', 'çœæ—¶', 'é«˜æ•ˆ', 'æ—¶é—´åˆ†é…', 'æ—¶é—´ç®¡ç†', 'é¢„çº¦å…¥å£',

    # åœ°ç‚¹/å®šä½
    'åæ ‡', 'å…¥å£', 'å‡ºå£', 'æ¥¼å±‚', 'æ¥¼', 'ä½ç½®', 'åœ°å€', 'æ–¹å‘', 'å·¦è½¬', 'å³è½¬', 'ç›´è¡Œ',
    'ç±³', 'å…¬é‡Œ', 'km', 'åˆ†é’Ÿ', 'min', 'å°æ—¶', 'h', 'æ­¥è¡Œ', 'è·ç¦»', 'é™„è¿‘', 'å‘¨è¾¹', 'å‘¨å›´',
    'åœ°æ ‡', 'åœ°å›¾', 'å…¨æ™¯å›¾', 'å®šä½', 'å‘¨è¾¹ç¯å¢ƒ', 'äº¤é€šä¾¿åˆ©', 'æ€ä¹ˆèµ°', 'å¦‚ä½•åˆ°è¾¾', 'æœ€è¿‘', 'æœ€è¿‘åœ°é“ç«™',
    'æ€ä¹ˆå»', 'æ€ä¹ˆåˆ°è¾¾',

    # æ—¶é—´
    'å¼€é—¨', 'å…³é—¨', 'è¥ä¸š', 'ä¼‘æ¯', 'èŠ‚å‡æ—¥', 'å·¥ä½œæ—¥', 'å‘¨ä¸€', 'å‘¨äºŒ', 'å‘¨ä¸‰', 'å‘¨å››', 'å‘¨äº”',
    'å‘¨å…­', 'å‘¨æ—¥', 'æ—©ä¸Š', 'ä¸Šåˆ', 'ä¸­åˆ', 'ä¸‹åˆ', 'æ™šä¸Š', 'å¤œé—´', 'å‡Œæ™¨', 'ç‚¹', 'æ—¶', 'åˆ†',
    'ç­‰å¾…æ—¶é—´', 'é™æµ', 'å³å°†', 'æ¯å‘¨', 'æ¯æœˆ', 'å…¨å¹´', 'å¯’æš‘å‡', 'å‡æœŸ', 'å¼€æ”¾æ—¥', 'æ’é˜Ÿ',

    # æ•°é‡/é™é‡
    'ç¬¬ä¸€', 'ç¬¬äºŒ', 'ç¬¬ä¸‰', 'å±‚', 'å·', 'ä¸ª', 'å®¶', 'å¤„', 'æ¬¡', 'é', 'è¶Ÿ', 'åº§', 'æ‰¹æ¬¡',
    'å…¨ç¨‹', 'å…¨ç¥¨', 'åŠç¥¨', 'ä½™ç¥¨', 'æ»¡å‘˜', 'é™é‡', 'äººæ•°', 'é™è´­', 'ä»…é™', 'ç»„å›¢',

    # æœåŠ¡ä¸è®¾æ–½
    'æœåŠ¡', 'è®¾æ–½', 'å«ç”Ÿé—´', 'æ´—æ‰‹é—´', 'å¯„å­˜', 'å……ç”µ', 'wifi', 'ä¼‘æ¯åŒº', 'åœè½¦åœº', 'å”®ç¥¨å¤„',
    'å–ç¥¨', 'é€€ç¥¨', 'æ¢ç¥¨', 'å„¿ç«¥ç¥¨', 'è€äººç¥¨', 'ä¼˜å…ˆé€šé“', 'æ— éšœç¢', 'å…è´¹å¯„å­˜', 'è¡Œæç®±',
    'å®‰æ£€å£', 'æœåŠ¡å°', 'å¯¼è§ˆ', 'è®²è§£å™¨', 'å’¨è¯¢å°', 'è‡ªåŠ©æœº', 'å‰å°', 'å¤§å…', 'è‡ªåŠ©å–ç¥¨',

    # ç¾é£Ÿ
    'é¤å…', 'å°åƒ', 'ç¾é£Ÿ', 'æ¨èèœ', 'ç‚¹å•', 'èœå“', 'äººå‡æ¶ˆè´¹', 'é¢„è®¢', 'é¢„å®š',
    'é¤ä½', 'ç”¨é¤é«˜å³°', 'æ’å·', 'å¤–å–', 'å ‚é£Ÿ', 'æ—©ç‚¹', 'åˆé¤', 'æ™šé¤', 'å¤œå®µ',
    'å…æ’é˜Ÿ', 'ä¸‹å•', 'ç‰¹äº§', 'ååƒ', 'ç”œå“', 'é¥®å“', 'é¥®æ–™', 'æ‰“åŒ…', 'å›¢è´­åˆ¸', 'å¿…å°',

    # ä½å®¿ç›¸å…³
    'é…’åº—', 'æ°‘å®¿', 'æ—…é¦†', 'å®¾é¦†', 'å®¢æ ˆ', 'é’å¹´æ—…ç¤¾', 'é’æ—…', 'æ—…ç¤¾', 'æ—…åº—', 'æ—…èˆ',
    'æ ‡é—´', 'å¤§åºŠæˆ¿', 'å•äººé—´', 'åŒäººé—´', 'å®¶åº­æˆ¿', 'å…¥ä½', 'é€€æˆ¿', 'é¢„è®¢æˆåŠŸ', 'ä½å®¿ä½“éªŒ', 'åºŠä½',
]

# æƒ…æ„Ÿå‹å…³é”®è¯ï¼ˆå»é‡ã€ä¼˜åŒ–ï¼‰
EMOTIONAL_KEYWORDS = [
    # æƒ…ç»ª/æ„Ÿå—ï¼ˆä¼˜åŒ–ç»„åˆï¼‰
    'å¤ªæ„ŸåŠ¨äº†', 'è¶…éœ‡æ’¼', 'å¤ªæƒŠå–œäº†', 'å¾ˆæµªæ¼«', 'è¢«æ²»æ„ˆ', 'æ³ªç›®äº†', 'å—¨ç¿»å¤©', 
    'ç¾å¾—ä¸è¡Œ', 'å¤ªæƒŠè‰³', 'å£®è§‚çš„', 'ç¾å“­äº†', 'å¤ªçˆ±äº†', 'ç»äº†', 'å¤ªç¾äº†', 'ç‚¸è£‚',
    'å¥½æƒŠå‘†', 'é†‰äº†å‘€', 'å¤ªè¿·äºº', 'å¿ƒåŠ¨çš„', 'å¥½æ¿€åŠ¨', 'å¤ªå…´å¥‹', 'å¼€å¿ƒæ­»', 'ä¹ç¿»å¤©',
    
    # ç½‘ç»œæµè¡Œè¡¨æ€/æ„Ÿå¹
    'OMG', 'wow', 'å“‡', 'å¤©å“ª', 'æˆ‘çš„å¤©', 'å¤ªæ£’äº†', 'amazing', 'incredible', 'fantastic',
    'å¥½èµ', 'ç‰›', 'ç¥ä»™', 'çˆ±æ­»äº†', 'ç§’æ€', 'é«˜èƒ½', 'å¿…å»', 'ç¬‘æ­»', 'ç¬‘ç–¯', 'å“ˆå–½',
    'å“ˆå“ˆå“ˆ', 'å˜»å˜»', 'å‘œå‘œ', '555', 'æ³ªå¥”', 'å“­äº†', 'å“­æ­»', 'ç¬‘å‡ºå£°', 'éœ‡æƒŠ', 'è¶…å¥½ç¬‘',
    'ç¾æ­»äº†', 'ç»ç¾', 'ç¾¡æ…•', 'é…¸äº†', 'ä¸Šç˜¾', 'åœä¸ä¸‹æ¥', 'å¤ªæœ‰æ„æ€äº†', 'åˆ·å±', 'ç«çˆ†', 'åˆ·çˆ†',

    # ä¸»è§‚è¯„ä»·/å®‰åˆ©
    'æˆ‘è§‰å¾—', 'æˆ‘è¶…çˆ±', 'å¼ºçƒˆæ¨è', 'è¶…çº§å–œæ¬¢', 'ç–¯ç‹‚å®‰åˆ©', 'ä¸€å®šè¦', 'å¿…é¡»', 'æœ€å–œæ¬¢', 'äººç”Ÿå¿…å»',
    'ä¸å®¹é”™è¿‡', 'éš¾ä»¥å¿˜æ€€', 'å€¼å¾—', 'å¤ªå€¼å¾—', 'çœŸå¿ƒæ¨è', 'äº²æµ‹æœ‰æ•ˆ', 'å¥½çœ‹åˆ°ä¸è¡Œ', 'çˆ†ç«', 'çˆ†æ¬¾',
    'é€†å¤©é¢œå€¼', 'çˆ†è¡¨', 'è¶…å¯çˆ±', 'èŒèŒå“’', 'é«˜é¢œå€¼', 'æ— æ•Œå¯çˆ±', 'é«˜åˆ†', 'äº”æ˜Ÿå¥½è¯„',

    # æ„Ÿå—åŠ¨è¯
    'æ„Ÿå—', 'ä½“éªŒ', 'äº«å—', 'è§¦åŠ¨', 'æ‰“åŠ¨', 'ç—´è¿·', 'é™¶é†‰', 'è¿·æ‹', 'ç€è¿·', 'è¢«æ²»æ„ˆ',
    'è¢«éœ‡æ’¼', 'è¢«å®‰åˆ©', 'è¢«ç§è‰', 'å…¥å‘', 'åœˆç²‰', 'å¥½è¯„', 'ç‹‚èµ', 'ç‚¹èµ', 'æ„Ÿå—åˆ°',
    'å—¨èµ·æ¥', 'ä¹ç¿»å¤©', 'æƒŠå‘†äº†', 'è¢«ç¾åˆ°', 'è¢«åœˆç²‰', 'ä¸Šå¤´', 'ç§è‰', 'æ‹”è‰',

    # æ‹Ÿå£°/å‰¯è¯/ç½‘ç»œæƒ…ç»ª
    'ç¬é—´', 'ç«‹åˆ»', 'é©¬ä¸Š', 'å±…ç„¶', 'ç«Ÿç„¶', 'æ„å¤–', 'æ²¡æƒ³åˆ°', 'çªç„¶', 'ä¸€ä¸‹å­', 'ä¸€ç§’çˆ±ä¸Š',
    'è¶…çº§', 'è¶…', 'å·¨', 'è¶…èµ', 'é¡¶çº§', 'ç‚¸è£‚', 'ç›´æ¥', 'ç‹ ç‹ ', 'çŒ›', 'å¥½å¼º', 'å¥½æ·±',
    'çˆ†ç‚¸', 'æ æ çš„', 'ä¸€çº§æ£’', 'æ— è¯­', 'å¥½å®¶ä¼™', 'æ•‘å‘½', 'ç»äº†', 'æ “Q', 'æ„Ÿæ…¨', 'ç ´é˜²',
    'å—‘åˆ°äº†', 'å˜´è§’ä¸Šæ‰¬', 'å“­äº†', 'å‘œå‘œå‘œ', '55555', 'æ¿€åŠ¨åˆ°ä¸è¡Œ', 'ç®€ç›´', 'éå¸¸', 'æè‡´',

    # äººç§°ä»£è¯/å·å…¥
    'æˆ‘', 'æˆ‘ä»¬', 'ä½ ', 'ä½ ä»¬', 'å’±ä»¬', 'å¤§å®¶', 'å°ä¼™ä¼´', 'æœ‹å‹ä»¬', 'å®è´ä»¬', 'å§å¦¹ä»¬',
    'ç”·å­©ä»¬', 'å°å§å¦¹', 'å“¥ä»¬å„¿', 'å…„å¼Ÿä»¬', 'å®¶äººä»¬', 'äº²ä»¬', 'å®¶ä¼™ä»¬'
]


class LanguageStyleDetector:
    def __init__(self):
        """åˆå§‹åŒ–è¯­è¨€é£æ ¼æ£€æµ‹å™¨"""
        self.functional_kw = set(FUNCTIONAL_KEYWORDS)
        self.emotional_kw = set(EMOTIONAL_KEYWORDS)
        self.style_threshold = 0.1  # æ··åˆå‹åˆ¤å®šé˜ˆå€¼
        self.batch_size = 5  # æ¯æ‰¹å¤„ç†çš„è§†é¢‘æ•°é‡
        
    def preprocess_text(self, text: str) -> str:
        """æ–‡æœ¬é¢„å¤„ç†"""
        if pd.isna(text) or text is None:
            return ""
        
        # å»é™¤å¤šä½™ç©ºæ ¼å’Œæ¢è¡Œï¼Œä½†ä¿ç•™åŸºæœ¬æ ¼å¼
        text = re.sub(r'\s+', ' ', str(text).strip())
        # ç»Ÿä¸€å¸¸è§è¡¨æƒ…ç¬¦å·ï¼Œä½†ä¸åˆ é™¤
        text = re.sub(r'[ğŸ˜€-ğŸ™]+', 'ğŸ˜Š', text)
        return text
    
    def split_sentences(self, text: str) -> List[str]:
        """åˆ†å¥å¤„ç†"""
        if not text:
            return []
        
        # ä¼˜åŒ–åˆ†å¥æ­£åˆ™ï¼Œæ›´å¥½åœ°å¤„ç†ç½‘ç»œæ–‡æœ¬
        sentences = re.split(r'([ã€‚ï¼ï¼Ÿï¼›\.\!\?;]+(?:[ğŸ˜€-ğŸ™\s]*)|(?:[ğŸ˜€-ğŸ™]+))', text)
        # åˆå¹¶åˆ†å¥ç»“æœ
        merged = []
        temp = ''
        for s in sentences:
            if s:
                if re.match(r'[ã€‚ï¼ï¼Ÿï¼›\.\!\?;]+[ğŸ˜€-ğŸ™\s]*|[ğŸ˜€-ğŸ™]+', s):
                    temp += s
                    if temp.strip():  # åªæ·»åŠ éç©ºå¥å­
                        merged.append(temp)
                    temp = ''
                else:
                    temp += s
        if temp and temp.strip():  # æ·»åŠ æœ€åä¸€ä¸ªéç©ºå¥å­
            merged.append(temp)
        
        return [s.strip() for s in merged if s.strip()]
    
    def count_keywords(self, sentence: str, keywords: set) -> int:
        """ç»Ÿè®¡å¥å­ä¸­å…³é”®è¯æ•°é‡ï¼Œè€ƒè™‘å¦å®šè¯"""
        words = jieba.lcut(sentence)
        count = 0
        has_negation = is_negation_present(sentence)
        
        for word in words:
            if word in keywords:
                if has_negation:
                    count -= 1  # å¦å®šè¯å­˜åœ¨æ—¶ï¼Œå…³é”®è¯è®¡æ•°ä¸ºè´Ÿ
                else:
                    count += 1
        return max(0, count)  # ç¡®ä¿ä¸ä¼šè¿”å›è´Ÿå€¼
    
    def has_numbers_and_units(self, sentence: str) -> bool:
        """æ£€æµ‹æ˜¯å¦åŒ…å«æ•°å­—å’Œå•ä½"""
        # æ£€æµ‹æ•°å­—+å•ä½çš„æ¨¡å¼
        patterns = [
            r'\d+\s*å…ƒ',
            r'\d+\s*å—',
            r'\d+\s*åˆ†é’Ÿ',
            r'\d+\s*å°æ—¶',
            r'\d+\s*ç±³',
            r'\d+\s*å…¬é‡Œ',
            r'\d+\s*km',
            r'\d+\s*min',
            r'\d+\s*h',
            r'\d+:\d+',  # æ—¶é—´æ ¼å¼
            r'\d+ç‚¹',
            r'\d+å·'
        ]
        
        for pattern in patterns:
            if re.search(pattern, sentence):
                return True
        return False
    
    def has_first_second_person(self, sentence: str) -> bool:
        """æ£€æµ‹æ˜¯å¦åŒ…å«ç¬¬ä¸€ã€äºŒäººç§°"""
        pronouns = ['æˆ‘', 'æˆ‘ä»¬', 'å’±ä»¬', 'ä½ ', 'ä½ ä»¬', 'å¤§å®¶', 'å°ä¼™ä¼´', 'æœ‹å‹ä»¬', 'å®è´ä»¬']
        for pronoun in pronouns:
            if pronoun in sentence:
                return True
        return False
    
    def has_emotional_punctuation(self, sentence: str) -> bool:
        """æ£€æµ‹æ˜¯å¦åŒ…å«æƒ…æ„Ÿæ ‡ç‚¹"""
        patterns = [
            r'ï¼{2,}',  # å¤šä¸ªæ„Ÿå¹å·
            r'å“ˆ{2,}',  # å“ˆå“ˆå“ˆ
            r'å‘œ{2,}',  # å‘œå‘œå‘œ
            r'555+',    # 555
            r'[ğŸ˜€-ğŸ™]'  # emojiè¡¨æƒ…
        ]
        
        for pattern in patterns:
            if re.search(pattern, sentence):
                return True
        return False
    
    def rule_override(self, sentence: str) -> Optional[int]:
        """è§„åˆ™è¦†ç›–åˆ¤æ–­ï¼Œè¿”å›0(åŠŸèƒ½)/1(æƒ…æ„Ÿ)æˆ–None"""
        func_count = self.count_keywords(sentence, self.functional_kw)
        emo_count = self.count_keywords(sentence, self.emotional_kw)
        
        # åŠŸèƒ½å‹è§„åˆ™åˆ¤æ–­
        if func_count >= 2:  # åŠŸèƒ½è¯â‰¥2ä¸ª
            return 0
        
        if func_count >= 1 and self.has_numbers_and_units(sentence):
            return 0
        
        # æƒ…æ„Ÿå‹è§„åˆ™åˆ¤æ–­
        if emo_count >= 1:
            return 1
            
        if self.has_first_second_person(sentence) and self.has_emotional_punctuation(sentence):
            return 1
            
        # å¦‚æœåŠŸèƒ½è¯å¤šäºæƒ…æ„Ÿè¯ä¸”æœ‰æ•°å­—å•ä½ï¼Œå€¾å‘åŠŸèƒ½å‹
        if func_count > emo_count and func_count >= 1 and self.has_numbers_and_units(sentence):
            return 0
            
        return None
    
    def simple_sentiment_predict(self, sentence: str) -> int:
        """ç®€å•çš„æƒ…æ„Ÿé¢„æµ‹ï¼ˆæ›¿ä»£BERTæ¨¡å‹ï¼‰"""
        # è®¡ç®—åŠŸèƒ½è¯å’Œæƒ…æ„Ÿè¯çš„æƒé‡
        func_count = self.count_keywords(sentence, self.functional_kw)
        emo_count = self.count_keywords(sentence, self.emotional_kw)
        
        # è€ƒè™‘å…¶ä»–ç‰¹å¾
        has_numbers = self.has_numbers_and_units(sentence)
        has_person = self.has_first_second_person(sentence)
        has_emo_punct = self.has_emotional_punctuation(sentence)
        
        # è®¡ç®—å¾—åˆ†
        func_score = func_count * 2
        if has_numbers:
            func_score += 1
            
        emo_score = emo_count * 2
        if has_person:
            emo_score += 1
        if has_emo_punct:
            emo_score += 1
            
        # è¿”å›é¢„æµ‹ç»“æœ
        if func_score > emo_score:
            return 0  # åŠŸèƒ½å‹
        else:
            return 1  # æƒ…æ„Ÿå‹
    
    def classify_sentence(self, sentence: str) -> int:
        """å¯¹å•ä¸ªå¥å­è¿›è¡Œåˆ†ç±»"""
        # é¦–å…ˆå°è¯•è§„åˆ™è¦†ç›–
        rule_result = self.rule_override(sentence)
        if rule_result is not None:
            return rule_result
        
        # å¦åˆ™ä½¿ç”¨ç®€å•é¢„æµ‹æ¨¡å‹
        return self.simple_sentiment_predict(sentence)
    
    def decide_ratios(self, labels: List[int], n_sent: int) -> Tuple[float, float, float, Optional[str]]:
        """è®¡ç®—ä¸‰ç§é£æ ¼çš„æ¯”ä¾‹ï¼Œè¿”å›æ··åˆç±»å‹æ ‡è®°"""
        if n_sent == 0:
            return 0.0, 0.0, 1.0, "imagery"
        
        if n_sent < 2:
            return 0.0, 0.0, 1.0, "imagery"
        
        # ç»Ÿè®¡å„ç±»å‹æ•°é‡
        func_count = labels.count(0)
        emo_count = labels.count(1)
        
        # è®¡ç®—æ¯”ä¾‹
        ratio_functional = func_count / n_sent
        ratio_emotional = emo_count / n_sent
        ratio_imagery = 1.0 - (ratio_functional + ratio_emotional)
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºæ··åˆå‹
        ratios = [ratio_functional, ratio_emotional, ratio_imagery]
        max_ratio = max(ratios)
        second_max = sorted(ratios, reverse=True)[1]
        
        if max_ratio - second_max < self.style_threshold:
            # è¿”å›æ··åˆç±»å‹æ ‡è®°
            if ratio_functional > 0.3 and ratio_emotional > 0.3:
                return ratio_functional, ratio_emotional, ratio_imagery, "mixed_func_emo"
            elif ratio_functional > 0.3 and ratio_imagery > 0.3:
                return ratio_functional, ratio_emotional, ratio_imagery, "mixed_func_img"
            elif ratio_emotional > 0.3 and ratio_imagery > 0.3:
                return ratio_functional, ratio_emotional, ratio_imagery, "mixed_emo_img"
        
        return ratio_functional, ratio_emotional, ratio_imagery, None
    
    def detect_style(self, content: str) -> Tuple[int, float, float, float, int, Optional[str]]:
        """æ£€æµ‹å•ä¸ªæ–‡æœ¬çš„è¯­è¨€é£æ ¼"""
        # é¢„å¤„ç†
        content = self.preprocess_text(content)
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºçº¯è¡¨æƒ…ç¬¦å·æˆ–ç©ºæ–‡æœ¬
        if is_text_emoji_or_symbol(content):
            return 0, 0.0, 0.0, 1.0, 2, "imagery"  # æ„è±¡å‹
        
        # æ£€æŸ¥æ–‡æœ¬é•¿åº¦
        if len(content.replace(' ', '')) < 5:  # é™ä½æœ€å°é•¿åº¦è¦æ±‚
            return 0, 0.0, 0.0, 1.0, 2, "imagery"  # æ„è±¡å‹
        
        # åˆ†å¥
        sentences = self.split_sentences(content)
        n_sent = len(sentences)
        
        # å¦‚æœå¥å­æ•°é‡å¤ªå°‘ä½†æœ‰å®é™…å†…å®¹ï¼Œä½œä¸ºå•å¥å¤„ç†
        if n_sent < 2 and content.strip():
            sentences = [content]
            n_sent = 1
        
        # å¯¹æ¯ä¸ªå¥å­è¿›è¡Œåˆ†ç±»
        labels = []
        for sentence in sentences:
            if sentence.strip():
                label = self.classify_sentence(sentence)
                labels.append(label)
        
        if not labels:  # å¦‚æœæ²¡æœ‰æœ‰æ•ˆå¥å­
            return 0, 0.0, 0.0, 1.0, 2, "imagery"
        
        # è®¡ç®—æ¯”ä¾‹å’Œæ··åˆç±»å‹
        ratio_functional, ratio_emotional, ratio_imagery, mixed_type = self.decide_ratios(labels, len(labels))
        
        # ç¡®å®šä¸»å¯¼é£æ ¼
        if mixed_type:
            dominant_style = 3  # æ··åˆå‹çš„æ ‡è¯†
        else:
            ratios = [ratio_functional, ratio_emotional, ratio_imagery]
            # å¦‚æœåŠŸèƒ½å‹å’Œæƒ…æ„Ÿå‹éƒ½å¾ˆä½ï¼Œåˆ¤å®šä¸ºæ„è±¡å‹
            if ratio_functional < 0.2 and ratio_emotional < 0.2:
                dominant_style = 2  # æ„è±¡å‹
            else:
                dominant_style = np.argmax(ratios)
        
        return n_sent, ratio_functional, ratio_emotional, ratio_imagery, dominant_style, mixed_type
    
    def process_dataset(self, input_path: str, output_path: str):
        """å¤„ç†æ•´ä¸ªæ•°æ®é›†
        
        Args:
            input_path: è¾“å…¥CSVæ–‡ä»¶è·¯å¾„ï¼Œéœ€åŒ…å«'video_id'å’Œ'text'åˆ—
            output_path: è¾“å‡ºCSVæ–‡ä»¶è·¯å¾„
        """
        print("æ­£åœ¨åŠ è½½æ•°æ®...")
        
        # è¯»å–è¾“å…¥æ•°æ®
        try:
            df = pd.read_csv(input_path, encoding='utf-8', low_memory=False)
        except UnicodeDecodeError:
            try:
                df = pd.read_csv(input_path, encoding='gbk', low_memory=False)
            except UnicodeDecodeError:
                df = pd.read_csv(input_path, encoding='gb18030', low_memory=False)
        
        # ç¡®ä¿video_idæ˜¯æ•´æ•°ç±»å‹
        df['video_id'] = pd.to_numeric(df['video_id'], errors='coerce').fillna(0).astype(int)
        
        print(f"å…±åŠ è½½ {len(df)} æ¡æ•°æ®")
        
        # æ£€æŸ¥å¿…éœ€çš„åˆ—
        required_cols = ['video_id', 'text']
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            raise ValueError(f"ç¼ºå°‘å¿…éœ€çš„åˆ—: {missing_cols}")
        
        # æŒ‰video_idæ’åºå¹¶å»é‡
        df = df.drop_duplicates(subset=['video_id']).sort_values('video_id')
        
        # è·å–ä¸Šæ¬¡å¤„ç†åˆ°çš„ID
        last_processed_id = get_last_processed_id(output_path)
        if last_processed_id > 0:
            print(f"ä»ID {last_processed_id} ç»§ç»­å¤„ç†")
            df = df[df['video_id'] > last_processed_id]
        
        if df.empty:
            print("æ²¡æœ‰æ–°æ•°æ®éœ€è¦å¤„ç†")
            return None
        
        # åˆå§‹åŒ–ç»“æœåˆ—è¡¨å’Œè¿›åº¦æ¡
        results = []
        total_batches = (len(df) + self.batch_size - 1) // self.batch_size
        pbar = tqdm(total=len(df), desc="å¤„ç†è¿›åº¦")
        
        # æŒ‰æ‰¹æ¬¡å¤„ç†æ•°æ®
        for start_idx in range(0, len(df), self.batch_size):
            batch_df = df.iloc[start_idx:start_idx + self.batch_size]
            batch_results = []
            
            for _, row in batch_df.iterrows():
                try:
                    video_id = row['video_id']
                    content = row['text']
                    
                    # æ£€æµ‹è¯­è¨€é£æ ¼
                    n_sent, ratio_func, ratio_emo, ratio_img, dominant, mixed_type = self.detect_style(content)
                    
                    # æ·»åŠ é£æ ¼åç§°
                    style_names = ['åŠŸèƒ½å‹', 'æƒ…æ„Ÿå‹', 'æ„è±¡å‹', 'æ··åˆå‹']
                    style_name = style_names[dominant] if dominant < len(style_names) else 'æœªçŸ¥'
                    
                    batch_results.append({
                        'video_id': video_id,
                        'text': content,
                        'style_name': style_name,
                        'n_sent': n_sent,
                        'ratio_functional': ratio_func,
                        'ratio_emotional': ratio_emo,
                        'ratio_imagery': ratio_img,
                        'dominant_style': dominant,
                        'mixed_type': mixed_type
                    })
                    
                    pbar.update(1)
                    
                except Exception as e:
                    print(f"\nå¤„ç†è§†é¢‘ID {video_id} æ—¶å‡ºé”™: {str(e)}")
                    continue
            
            # æ¯æ‰¹æ¬¡ä¿å­˜ä¸€æ¬¡ç»“æœ
            if batch_results:
                batch_df = pd.DataFrame(batch_results)
                
                # è°ƒæ•´åˆ—çš„é¡ºåº
                columns_order = [
                    'video_id',           # åŸå§‹ID
                    'text',              # åŸå§‹æ–‡æ¡ˆ
                    'style_name',        # é£æ ¼åç§°
                    'dominant_style',    # é£æ ¼ç¼–å·
                    'ratio_functional',  # åŠŸèƒ½å‹æ¯”ä¾‹
                    'ratio_emotional',   # æƒ…æ„Ÿå‹æ¯”ä¾‹
                    'ratio_imagery',     # æ„è±¡å‹æ¯”ä¾‹
                    'mixed_type',        # æ··åˆç±»å‹
                    'n_sent'            # å¥å­æ•°é‡
                ]
                batch_df = batch_df[columns_order]
                
                # å¦‚æœæ–‡ä»¶å­˜åœ¨ï¼Œåˆ™åˆå¹¶ç»“æœ
                if os.path.exists(output_path):
                    try:
                        existing_df = pd.read_csv(output_path, encoding='utf-8')
                    except UnicodeDecodeError:
                        try:
                            existing_df = pd.read_csv(output_path, encoding='gbk')
                        except UnicodeDecodeError:
                            existing_df = pd.read_csv(output_path, encoding='gb18030')
                    
                    # ç¡®ä¿video_idæ˜¯æ•´æ•°ç±»å‹
                    existing_df['video_id'] = pd.to_numeric(existing_df['video_id'], errors='coerce').fillna(0).astype(int)
                    # ç§»é™¤å¯èƒ½å­˜åœ¨çš„é‡å¤é¡¹
                    existing_df = existing_df[~existing_df['video_id'].isin(batch_df['video_id'])]
                    batch_df = pd.concat([existing_df, batch_df])
                
                # æŒ‰video_idæ’åºå¹¶ä¿å­˜
                batch_df = batch_df.sort_values('video_id')
                batch_df.to_csv(output_path, index=False, encoding='utf-8-sig')  # ä½¿ç”¨å¸¦BOMçš„UTF-8ç¼–ç 
                
                # æ›´æ–°æ€»ç»“æœåˆ—è¡¨
                results.extend(batch_results)
        
        pbar.close()
        
        # æ‰“å°æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
        if results:
            result_df = pd.DataFrame(results)
            print("\næœ¬æ¬¡å¤„ç†çš„è¯­è¨€é£æ ¼åˆ†å¸ƒ:")
            style_stats = result_df['style_name'].value_counts()
            for style, count in style_stats.items():
                print(f"{style}: {count} æ¡ ({count/len(result_df)*100:.1f}%)")
            
            if not result_df['mixed_type'].isna().all():
                print("\næœ¬æ¬¡å¤„ç†çš„æ··åˆç±»å‹åˆ†å¸ƒ:")
                mixed_counts = result_df['mixed_type'].value_counts()
                for mixed_type, count in mixed_counts.items():
                    if pd.notna(mixed_type):
                        print(f"{mixed_type}: {count} æ¡ ({count/len(result_df)*100:.1f}%)")
            
            # æ‰“å°æ€»ä½“ç»Ÿè®¡ä¿¡æ¯
            if os.path.exists(output_path):
                try:
                    total_df = pd.read_csv(output_path, encoding='utf-8-sig')
                except UnicodeDecodeError:
                    try:
                        total_df = pd.read_csv(output_path, encoding='gbk')
                    except UnicodeDecodeError:
                        total_df = pd.read_csv(output_path, encoding='gb18030')
                
                print("\nç´¯è®¡å¤„ç†çš„è¯­è¨€é£æ ¼åˆ†å¸ƒ:")
                total_style_stats = total_df['style_name'].value_counts()
                for style, count in total_style_stats.items():
                    print(f"{style}: {count} æ¡ ({count/len(total_df)*100:.1f}%)")
        
        return pd.DataFrame(results) if results else None

def main():
    """ä¸»å‡½æ•°"""
    # æŒ‡å®šè¾“å…¥è¾“å‡ºè·¯å¾„
    input_path = r"xxxxxxxxxxxxxxxxxxx"
    output_path = r"xxxxxxxxxxxxxxxxxxx"
    
    # åˆ›å»ºé˜²ä¼‘çœ å¯¹è±¡
    prevent_sleep = PreventSleep()
    
    try:
        # å¯ç”¨é˜²ä¼‘çœ 
        prevent_sleep.enable()
        
        # åˆ›å»ºæ£€æµ‹å™¨å¹¶å¤„ç†æ•°æ®
        detector = LanguageStyleDetector()
        result_df = detector.process_dataset(input_path, output_path)
        
        if result_df is not None:
            print("\nå¤„ç†æˆåŠŸå®Œæˆï¼")
        
    except Exception as e:
        print(f"å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
    
    finally:
        # æ¢å¤é»˜è®¤ç”µæºè®¾ç½®
        prevent_sleep.disable()

if __name__ == "__main__":
    main() 